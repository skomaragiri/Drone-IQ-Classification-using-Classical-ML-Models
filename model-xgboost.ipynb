{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07ea668",
   "metadata": {},
   "source": [
    "## *Prep*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a986a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# The following five lines ensure that we reload the preprocessing functions \n",
    "# everytime this cell is called\n",
    "import importlib\n",
    "import helper_files.preprocessing as preprocFuncts\n",
    "import helper_files.util as util\n",
    "import parameters as MyParams\n",
    "importlib.reload(preprocFuncts)\n",
    "importlib.reload(util)\n",
    "importlib.reload(MyParams)\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "\n",
    "# DERIVATION PARAMETERS\n",
    "WINDOW_LEN = MyParams.WINDOW_LEN # measured in IQ samples\n",
    "OVERLAP = MyParams.OVERLAP\n",
    "NUM_FEATURES = MyParams.NUM_FEATURES\n",
    "\n",
    "# DATA PARAMETERS\n",
    "NUM_TRAINING_FILES = MyParams.NUM_TRAINING_FILES # how many files in the saved numpy data for training\n",
    "NUM_EVALUATION_FILES = MyParams.NUM_EVALUATION_FILES # how many files in the saved numpy data for evaluation\n",
    "MAX_FILES = MyParams.MAX_FILES # if not using the saved numpy data, this is the max files to intake\n",
    "USE_SAVED_DATA = MyParams.USE_SAVED_DATA # True = used the saved .npy file data instead of re-deriving the features again\n",
    "SAVE_METRICS_TO_FILE = MyParams.SAVE_METRICS_TO_FILE\n",
    "TRAINING_DATASET = MyParams.TRAINING_DATASET\n",
    "EVAL_DATASET = MyParams.EVAL_DATASET\n",
    "\n",
    "FEATURES_TO_USE = MyParams.FEATURES_TO_USE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83869748",
   "metadata": {},
   "source": [
    "## *Loading training dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f47c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(preprocFuncts)\n",
    "importlib.reload(util)\n",
    "importlib.reload(MyParams)\n",
    "\n",
    "# carlos' Mac\n",
    "# data_dir = '/Users/carlos_1/Documents/GitHub/RFML-Code/RFML_Combined_Dataset_2025/RFML_Drone_Dataset_2025/old_drone_full_annotated_dataset/RFML_Old_Drone_Training_Dataset/*'\n",
    "# uav-cyberlab-rfml laptop\n",
    "data_dir = f'/home/uav-cyberlab-rfml/RFML/test_{TRAINING_DATASET}_training'\n",
    "print(\"Pulling from directory: \", data_dir)\n",
    "\n",
    "training_derived_samples, training_labels = preprocFuncts.preprocessFiles(\n",
    "    data_dir, \n",
    "    postfix=f\"train_{NUM_FEATURES}ftrs_{NUM_TRAINING_FILES}files_{WINDOW_LEN}win_{'0' + str(int(OVERLAP * 100))}over{\"_\"+TRAINING_DATASET if TRAINING_DATASET != \"\" else \"\"}\", \n",
    "    features_to_use=FEATURES_TO_USE,\n",
    "    window_len=WINDOW_LEN,\n",
    "    overlap=OVERLAP,\n",
    "    saved_data=USE_SAVED_DATA, \n",
    "    max_files=MAX_FILES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf036b16",
   "metadata": {},
   "source": [
    "## *Preprocessing training dataset*\n",
    "\n",
    "The following cell removes labels that should not be used---like annotations that were not labeled, or the 'Burst' label, or the Ruko F11 pro labels because it does not appear in the evaluation dataset. \n",
    "\n",
    "The cell also applies an encoder to the labels to use for fitting and predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cc850",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==BEFORE BALANCING========\")\n",
    "util.display(training_derived_samples, training_labels)\n",
    "\n",
    "training_derived_samples, training_labels = preprocFuncts.balanceByMedian(training_derived_samples, training_labels, unlabeled_downsampling=70_000)\n",
    "\n",
    "print(\"\\n==AFTER BALANCING========\")\n",
    "util.display(training_derived_samples, training_labels)\n",
    "\n",
    "\n",
    "print(f\"Number of samples: {training_derived_samples.shape[0]:,}\")\n",
    "print(f\"Number of labels: {len(training_labels):,}\")\n",
    "\n",
    "remove_labels = [\n",
    "    'Burst',\n",
    "    '',\n",
    "    # 'Ruko_F11_pro_UL',\n",
    "    # 'HS100_Downlink',\n",
    "]\n",
    "\n",
    "# Build mask: True = keep, False = remove\n",
    "mask = ~np.isin(training_labels, remove_labels)\n",
    "\n",
    "# Apply mask\n",
    "training_derived_samples = training_derived_samples[mask]\n",
    "training_labels = training_labels[mask]\n",
    "training_labels_strings = training_labels # save the strings\n",
    "\n",
    "print(f\"\\nAfter removing unnecessary labels:\")\n",
    "print(f\"Number of samples: {training_derived_samples.shape[0]:,}\")\n",
    "print(f\"Number of labels: {len(training_labels):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36dc9fa",
   "metadata": {},
   "source": [
    "## *Loading evaluation dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(preprocFuncts)\n",
    "importlib.reload(util)\n",
    "importlib.reload(MyParams)\n",
    "\n",
    "# carlos' Mac\n",
    "# data_dir = '/Users/carlos_1/Documents/GitHub/RFML-Code/RFML_Combined_Dataset_2025/RFML_Drone_Dataset_2025/old_drone_full_annotated_dataset/RFML_Old_Drone_Eval_data/*'\n",
    "# uav-cyberlab-rfml laptop\n",
    "data_dir = f'/home/uav-cyberlab-rfml/RFML/test_{EVAL_DATASET}_eval'\n",
    "print(\"Pulling from directory: \", data_dir)\n",
    "\n",
    "\n",
    "test_derived_samples, test_labels = preprocFuncts.preprocessFiles(\n",
    "    data_dir, \n",
    "    postfix=f\"eval_{NUM_FEATURES}ftrs_{NUM_EVALUATION_FILES}files_{WINDOW_LEN}win_{'0' + str(int(OVERLAP * 100))}over{\"_\"+EVAL_DATASET if EVAL_DATASET != \"\" else \"\"}\", \n",
    "    features_to_use=FEATURES_TO_USE,\n",
    "    window_len=WINDOW_LEN,\n",
    "    overlap=OVERLAP,\n",
    "    saved_data=USE_SAVED_DATA, \n",
    "    max_files=MAX_FILES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b66dfc",
   "metadata": {},
   "source": [
    "## *Preprocessing evaluation dataset*\n",
    "First I need to remove labels that were in the evaluation set but not in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5971ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==BEFORE BALANCING========\")\n",
    "util.display(test_derived_samples, test_labels)\n",
    "\n",
    "test_derived_samples, test_labels = preprocFuncts.balanceByMedian(test_derived_samples, test_labels, unlabeled_downsampling=7_000)\n",
    "print(\"\\n==AFTER BALANCING========\")\n",
    "util.display(test_derived_samples, training_labels)\n",
    "\n",
    "\n",
    "print(f\"Number of samples: {test_derived_samples.shape[0]:,}\")\n",
    "print(f\"Number of labels: {test_labels.shape[0]:,}\")\n",
    "\n",
    "# remove labels that are in the test_labels but not in y_train\n",
    "remove_labels = np.setdiff1d(test_labels, training_labels_strings).tolist()\n",
    "print(remove_labels)\n",
    "\n",
    "# Build mask: True = keep, False = remove\n",
    "mask = ~np.isin(test_labels, remove_labels)\n",
    "\n",
    "# Apply mask\n",
    "test_derived_samples = test_derived_samples[mask]\n",
    "test_labels = test_labels[mask]\n",
    "\n",
    "print(f\"\\nAfter removing unnecessary labels:\")\n",
    "print(f\"Number of samples: {test_derived_samples.shape[0]:,}\")\n",
    "print(f\"Number of labels: {test_labels.shape[0]:,}\")\n",
    "\n",
    "y_strings = test_labels # store the string version of the labels before they get encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a92e723",
   "metadata": {},
   "source": [
    "## *Fitting and prediction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for proper overlap between testing and training\n",
    "\n",
    "unique_train_classes = set(np.unique(training_labels))\n",
    "unique_num_test_classes  = set(np.unique(test_labels))\n",
    "\n",
    "print(\"Number of training classes:\\t\", len(unique_train_classes))\n",
    "print(\"Number of testing classes:\\t\", len(unique_num_test_classes))\n",
    "print(\"Overlap:\\t\\t\\t\", len(unique_train_classes & unique_num_test_classes))\n",
    "print(\"Only-in-test :\", sorted(unique_num_test_classes - unique_train_classes)[:20])\n",
    "print(\"Only-in-train:\", sorted(unique_train_classes - unique_num_test_classes)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb107af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Ensure arrays\n",
    "X_train = np.asarray(training_derived_samples)\n",
    "y_train = np.asarray(training_labels)\n",
    "X_test = np.asarray(test_derived_samples)\n",
    "y_test = np.asarray(test_labels)\n",
    "\n",
    "# Encode string labels to integers\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    # num_class=len(le.classes_), \n",
    "    eval_metric='mlogloss',\n",
    "    # use_label_encoder=False,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.3,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train[0][:2]}\")\n",
    "print(f\"Training labels: {y_train[:5]}\")\n",
    "print(f\"Training samples type: {X_train.dtype}\")\n",
    "print(f\"Training labels type: {y_train.dtype}\")\n",
    "print(f\"Training samples size: {len(X_train)}\")\n",
    "print(f\"Training labels size: {len(y_train)}\")\n",
    "print(f\"Training samples shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ef75b",
   "metadata": {},
   "source": [
    "## *Metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c0aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "importlib.reload(preprocFuncts)\n",
    "importlib.reload(util)\n",
    "\n",
    "base_name='xgb'\n",
    "model=\"XGBoost\"\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "perc_accuracy = accuracy * 100\n",
    "cr = classification_report(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "util.printMetrics(\n",
    "    base_name=base_name + \"_metrics\", \n",
    "    model=model, \n",
    "    perc_accuracy=perc_accuracy, \n",
    "    notes=f\"\",\n",
    "    cr=cr,\n",
    "    cm=cm,\n",
    "    labels=y_strings,\n",
    "    max_files=MAX_FILES,\n",
    "    window_size=WINDOW_LEN,\n",
    "    overlap=OVERLAP,\n",
    "    num_features=NUM_FEATURES,\n",
    "    num_training_files=NUM_TRAINING_FILES,\n",
    "    num_evaluation_files=NUM_EVALUATION_FILES,\n",
    "    training_dataset=TRAINING_DATASET,\n",
    "    eval_dataset=EVAL_DATASET,\n",
    ")\n",
    "\n",
    "\n",
    "if SAVE_METRICS_TO_FILE:\n",
    "    util.saveMetricsToFile(\n",
    "        base_name=base_name + \"_metrics\", \n",
    "        model=model, \n",
    "        perc_accuracy=perc_accuracy, \n",
    "        notes=\"Using the fixed length sample size\",\n",
    "        cr=cr,\n",
    "        cm=cm,\n",
    "        labels=y_strings,\n",
    "        max_files=MAX_FILES,\n",
    "        window_size=WINDOW_LEN,\n",
    "        overlap=OVERLAP,\n",
    "        num_features = NUM_FEATURES,\n",
    "        num_training_files = NUM_TRAINING_FILES,\n",
    "        num_evaluation_files = NUM_EVALUATION_FILES,\n",
    "        training_dataset=TRAINING_DATASET,\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the confusion matrix\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "sn.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_strings), yticklabels=np.unique(y_strings))\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix Heatmap for {model} Predictions')\n",
    "\n",
    "if SAVE_METRICS_TO_FILE:\n",
    "    plt.savefig(f\"./metrics/{base_name}_cm_{(perc_accuracy * 100):.0f}.png\", bbox_inches='tight', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f07bc",
   "metadata": {},
   "source": [
    "The following cell prints Cohen's Kappa and Matthew's Correlation Coefficient\n",
    "* Cohen's Kappa (Îº) is a statistic measuring agreement between two categorical raters (or one rater at two times) beyond what's expected by chance, correcting for random agreement, and is used for inter-rater reliability in fields like machine learning. It ranges from -1 (total disagreement) to +1 (perfect agreement), with 0 meaning agreement is purely random.\n",
    "\n",
    "* The Matthews Correlation Coefficient (MCC) is a robust metric in machine learning for evaluating binary/multiclass classification, measuring correlation between actual and predicted classes, ranging from -1 (perfect inverse) to +1 (perfect prediction), with 0 being random; it's especially valuable for imbalanced datasets as it uses all four confusion matrix values (TP, TN, FP, FN) for a balanced performance score, unlike simpler metrics that can be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d819b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"MCC:\", matthews_corrcoef(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
