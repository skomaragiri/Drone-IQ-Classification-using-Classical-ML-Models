{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07ea668",
   "metadata": {},
   "source": [
    "## *Prep*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a986a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# The following five lines ensure that we reload the preprocessing functions \n",
    "# everytime this cell is called\n",
    "import importlib\n",
    "import helper_files.preprocessing as preprocFuncts\n",
    "import helper_files.util as util\n",
    "import parameters as MyParams\n",
    "importlib.reload(preprocFuncts)\n",
    "importlib.reload(util)\n",
    "importlib.reload(MyParams)\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "\n",
    "# DERIVATION PARAMETERS\n",
    "WINDOW_LEN = MyParams.WINDOW_LEN # measured in IQ samples\n",
    "OVERLAP = MyParams.OVERLAP\n",
    "NUM_FEATURES = MyParams.NUM_FEATURES\n",
    "\n",
    "# DATA PARAMETERS\n",
    "TRAINING_DATA_DIR = MyParams.training_data_dir\n",
    "EVAL_DATA_DIR = MyParams.eval_data_dir\n",
    "NUM_TRAINING_FILES = MyParams.NUM_TRAINING_FILES # how many files in the saved numpy data for training\n",
    "NUM_EVALUATION_FILES = MyParams.NUM_EVALUATION_FILES # how many files in the saved numpy data for evaluation\n",
    "MAX_FILES = MyParams.MAX_FILES # if not using the saved numpy data, this is the max files to intake\n",
    "USE_SAVED_DATA = MyParams.USE_SAVED_DATA # True = used the saved .npy file data instead of re-deriving the features again\n",
    "SAVE_METRICS_TO_FILE = MyParams.SAVE_METRICS_TO_FILE\n",
    "TRAINING_DATASET = MyParams.TRAINING_DATASET\n",
    "EVAL_DATASET = MyParams.EVAL_DATASET\n",
    "\n",
    "FEATURES_TO_USE = MyParams.FEATURES_TO_USE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83869748",
   "metadata": {},
   "source": [
    "## *Loading training dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26f47c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling from directory:  /home/uav-cyberlab-rfml/RFML/test-dataset/test_dji_20mhz_comms_training\n",
      "Converted 62,500,000 IQ samples to numpy array. Shape: (62500000,)\n",
      "Converted 1 files\n",
      "Extracting features now!\n",
      "The shape of patches: (60, 2, 16, 64)\n",
      "The shape of time_starts: (60,)\n",
      "The shape of freq_starts: (2,)\n",
      "The time_start: 0\n",
      "The freq_start: 0\n",
      "The time_start: 1024\n",
      "The freq_start: 1024\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPulling from directory: \u001b[39m\u001b[33m\"\u001b[39m, TRAINING_DATA_DIR)\n\u001b[32m      7\u001b[39m training_derived_samples, training_labels = preprocFuncts.preprocessFiles(\n\u001b[32m      8\u001b[39m     TRAINING_DATA_DIR, \n\u001b[32m      9\u001b[39m     postfix=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_FEATURES\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mftrs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_TRAINING_FILES\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mfiles_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWINDOW_LEN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mwin_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(OVERLAP\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m100\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mover\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m+TRAINING_DATASET\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mTRAINING_DATASET\u001b[38;5;250m \u001b[39m!=\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     max_files=MAX_FILES,\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mtraining_derived_samples\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m <= \u001b[32m0\u001b[39m):\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mUserWarning\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mArray is empty\u001b[39m\u001b[33m\"\u001b[39m) \n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "importlib.reload(preprocFuncts)\n",
    "importlib.reload(util)\n",
    "importlib.reload(MyParams)\n",
    "\n",
    "print(\"Pulling from directory: \", TRAINING_DATA_DIR)\n",
    "\n",
    "training_derived_samples, training_labels = preprocFuncts.preprocessFiles(\n",
    "    TRAINING_DATA_DIR, \n",
    "    postfix=f\"train_{NUM_FEATURES}ftrs_{NUM_TRAINING_FILES}files_{WINDOW_LEN}win_{'0' + str(int(OVERLAP * 100))}over{\"_\"+TRAINING_DATASET if TRAINING_DATASET != \"\" else \"\"}\", \n",
    "    features_to_use=FEATURES_TO_USE,\n",
    "    window_len=WINDOW_LEN,\n",
    "    freq_bin_width=12,\n",
    "    overlap=OVERLAP,\n",
    "    saved_data=USE_SAVED_DATA, \n",
    "    max_files=MAX_FILES,\n",
    ")\n",
    "\n",
    "if (training_derived_samples.size <= 0):\n",
    "    raise UserWarning(\"Array is empty\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf036b16",
   "metadata": {},
   "source": [
    "## *Preprocessing training dataset*\n",
    "\n",
    "The following cell removes labels that should not be used---like annotations that were not labeled, or the 'Burst' label, or the Ruko F11 pro labels because it does not appear in the evaluation dataset. \n",
    "\n",
    "The cell also applies an encoder to the labels to use for fitting and predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cc850",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==BEFORE BALANCING========\")\n",
    "util.display(training_derived_samples, training_labels)\n",
    "\n",
    "training_derived_samples, training_labels = preprocFuncts.balanceByMedian(training_derived_samples, training_labels, unlabeled_downsampling=70_000)\n",
    "\n",
    "print(\"\\n==AFTER BALANCING========\")\n",
    "util.display(training_derived_samples, training_labels)\n",
    "\n",
    "\n",
    "\n",
    "X_train = training_derived_samples\n",
    "y_train = training_labels\n",
    "\n",
    "print(f\"Number of samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Number of labels: {len(y_train):,}\")\n",
    "\n",
    "remove_labels = [\n",
    "    'Burst',\n",
    "    '',\n",
    "    'Ruko_F11_pro_UL',\n",
    "    'HS100_Downlink',\n",
    "]\n",
    "\n",
    "# Build mask: True = keep, False = remove\n",
    "mask = ~np.isin(y_train, remove_labels)\n",
    "\n",
    "# Apply mask\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "y_train_strings = y_train\n",
    "\n",
    "print(f\"\\nAfter removing unnecessary labels:\")\n",
    "print(f\"Number of samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Number of labels: {len(y_train):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36dc9fa",
   "metadata": {},
   "source": [
    "## *Loading evaluation dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(preprocFuncts)\n",
    "importlib.reload(util)\n",
    "importlib.reload(MyParams)\n",
    "\n",
    "# carlos' Mac\n",
    "# data_dir = '/Users/carlos_1/Documents/GitHub/RFML-Code/RFML_Combined_Dataset_2025/RFML_Drone_Dataset_2025/old_drone_full_annotated_dataset/RFML_Old_Drone_Eval_data/*'\n",
    "# uav-cyberlab-rfml laptop\n",
    "data_dir = f'/home/uav-cyberlab-rfml/RFML/test_{EVAL_DATASET}_eval'\n",
    "print(\"Pulling from directory: \", data_dir)\n",
    "\n",
    "\n",
    "test_derived_samples, test_labels = preprocFuncts.preprocessFiles(\n",
    "    data_dir, \n",
    "    postfix=f\"eval_{NUM_FEATURES}ftrs_{NUM_EVALUATION_FILES}files_{WINDOW_LEN}win_{'0' + str(int(OVERLAP * 100))}over{\"_\"+EVAL_DATASET if EVAL_DATASET != \"\" else \"\"}\", \n",
    "    features_to_use=FEATURES_TO_USE,\n",
    "    window_len=WINDOW_LEN,\n",
    "    overlap=OVERLAP,\n",
    "    saved_data=USE_SAVED_DATA, \n",
    "    max_files=MAX_FILES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b66dfc",
   "metadata": {},
   "source": [
    "## *Preprocessing evaluation dataset*\n",
    "First I need to remove labels that were in the evaluation set but not in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5971ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==BEFORE BALANCING========\")\n",
    "util.display(test_derived_samples, test_labels)\n",
    "\n",
    "test_derived_samples, test_labels = preprocFuncts.balanceByMedian(test_derived_samples, test_labels, unlabeled_downsampling=7_000)\n",
    "print(\"\\n==AFTER BALANCING========\")\n",
    "util.display(test_derived_samples, training_labels)\n",
    "\n",
    "\n",
    "X_test = test_derived_samples\n",
    "y_test = test_labels\n",
    "\n",
    "\n",
    "print(f\"Number of samples: {X_test.shape[0]:,}\")\n",
    "print(f\"Number of labels: {y_test.shape[0]:,}\")\n",
    "\n",
    "# remove labels that are in the y_test but not in y_train\n",
    "remove_labels = np.setdiff1d(y_test, y_train_strings).tolist()\n",
    "print(remove_labels)\n",
    "\n",
    "# Build mask: True = keep, False = remove\n",
    "mask = ~np.isin(y_test, remove_labels)\n",
    "\n",
    "# Apply mask\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print(f\"\\nAfter removing unnecessary labels:\")\n",
    "print(f\"Number of samples: {X_test.shape[0]:,}\")\n",
    "print(f\"Number of labels: {y_test.shape[0]:,}\")\n",
    "\n",
    "y_strings = y_test # store the string version of the labels before they get encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a92e723",
   "metadata": {},
   "source": [
    "## *Fitting and prediction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for proper overlap between testing and training\n",
    "\n",
    "train_classes = set(np.unique(y_train))\n",
    "test_classes  = set(np.unique(y_test))\n",
    "\n",
    "print(\"Train classes:\", len(train_classes))\n",
    "print(\"Test classes :\", len(test_classes))\n",
    "print(\"Overlap      :\", len(train_classes & test_classes))\n",
    "print(\"Only-in-test :\", sorted(test_classes - train_classes)[:20])\n",
    "print(\"Only-in-train:\", sorted(train_classes - test_classes)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fcdf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "# Ensure arrays\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "# 1. Scale features (important for GNB)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "power_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "X_train_scaled = power_transformer.fit_transform(X_train_scaled)\n",
    "\n",
    "# 2. Train GNB\n",
    "gnb = GaussianNB(var_smoothing=1e-9)\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "print(f\"Fit ✔︎\")\n",
    "\n",
    "y_pred = None\n",
    "if X_test is not None and y_test is not None:\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled = power_transformer.transform(X_test_scaled)\n",
    "    y_pred = gnb.predict(X_test_scaled)\n",
    "    print(f\"Predicted ✔︎\")\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {acc:.2f}%\")\n",
    "else: \n",
    "    print(\"Check the testing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ef75b",
   "metadata": {},
   "source": [
    "## *Metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c0aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "importlib.reload(preprocFuncts)\n",
    "importlib.reload(util)\n",
    "\n",
    "base_name='gnb'\n",
    "model=\"Gaussian Naive-Bayes\"\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "perc_accuracy = accuracy * 100\n",
    "cr = classification_report(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "util.printMetrics(\n",
    "    base_name=base_name + \"_metrics\", \n",
    "    model=model, \n",
    "    perc_accuracy=perc_accuracy, \n",
    "    notes=f\"\",\n",
    "    cr=cr,\n",
    "    cm=cm,\n",
    "    labels=y_strings,\n",
    "    max_files=MAX_FILES,\n",
    "    window_size=WINDOW_LEN,\n",
    "    overlap=OVERLAP,\n",
    "    num_features=NUM_FEATURES,\n",
    "    num_training_files=NUM_TRAINING_FILES,\n",
    "    num_evaluation_files=NUM_EVALUATION_FILES,\n",
    "    training_dataset=TRAINING_DATASET,\n",
    "    eval_dataset=EVAL_DATASET,\n",
    ")\n",
    "\n",
    "\n",
    "if SAVE_METRICS_TO_FILE:\n",
    "    util.saveMetricsToFile(\n",
    "        base_name=base_name + \"_metrics\", \n",
    "        model=model, \n",
    "        perc_accuracy=perc_accuracy, \n",
    "        notes=\"Using the fixed length sample size\",\n",
    "        cr=cr,\n",
    "        cm=cm,\n",
    "        labels=y_strings,\n",
    "        max_files=MAX_FILES,\n",
    "        window_size=WINDOW_LEN,\n",
    "        overlap=OVERLAP,\n",
    "        num_features = NUM_FEATURES,\n",
    "        num_training_files = NUM_TRAINING_FILES,\n",
    "        num_evaluation_files = NUM_EVALUATION_FILES,\n",
    "        training_dataset=TRAINING_DATASET,\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the confusion matrix\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "sn.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_strings), yticklabels=np.unique(y_strings))\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix Heatmap for {model} Predictions')\n",
    "\n",
    "if SAVE_METRICS_TO_FILE:\n",
    "    plt.savefig(f\"./metrics/{base_name}_cm_{(perc_accuracy * 100):.0f}.png\", bbox_inches='tight', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f07bc",
   "metadata": {},
   "source": [
    "The following cell prints Cohen's Kappa and Matthew's Correlation Coefficient\n",
    "* Cohen's Kappa (κ) is a statistic measuring agreement between two categorical raters (or one rater at two times) beyond what's expected by chance, correcting for random agreement, and is used for inter-rater reliability in fields like machine learning. It ranges from -1 (total disagreement) to +1 (perfect agreement), with 0 meaning agreement is purely random.\n",
    "\n",
    "* The Matthews Correlation Coefficient (MCC) is a robust metric in machine learning for evaluating binary/multiclass classification, measuring correlation between actual and predicted classes, ranging from -1 (perfect inverse) to +1 (perfect prediction), with 0 being random; it's especially valuable for imbalanced datasets as it uses all four confusion matrix values (TP, TN, FP, FN) for a balanced performance score, unlike simpler metrics that can be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d819b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"MCC:\", matthews_corrcoef(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
